import streamlit as st
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI

from components.langchain.init_llm import initialize_open_ai_llm
from components.langchain.init_memory import initialize_conversation_memory
from components.langchain.site_parser import parse_by_sitemap_xml
from components.pages.common.session import set_session
from components.pages.sitegpt.event import on_button_click
from components.pages.sitegpt.prompt import find_answer_prompt, pick_answer_prompt

# ìˆ˜ì •ì‚¬í•­
# 1. history ì¶”ê°€
# 2. memory ì¶”ê°€ - ì²˜ë¦¬ì™„ë£Œ
# 3. ui ë¥¼ document gpt ì™€ ë™ì¼ í•˜ê²Œ ì„¤ì •
st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ–¥ï¸",
)


@st.cache_resource(show_spinner="Initialize LLM..")
def init_open_ai() -> ChatOpenAI:
    return initialize_open_ai_llm()


llm = init_open_ai()


@st.cache_resource(show_spinner="Fetching document by WebSite...")
def find_docs_by_sitemap(url_param: str, url_filter_param: str) -> VectorStoreRetriever:
    filter_arr = []

    if url_filter_param:
        filter_arr.append(fr"^(.*\/{url_filter_param}.*\/)")

    return parse_by_sitemap_xml(url_param, filter_arr)


@st.cache_resource
def init_memory() -> ConversationSummaryBufferMemory:
    return initialize_conversation_memory(chat_model=llm, memory_key='chat_history', return_messages=True)


memory = init_memory()


def __get_answers(inputs):
    docs = inputs["docs"]
    question = inputs["question"]
    answers_chain = RunnablePassthrough.assign(chat_history=__load_memory) | find_answer_prompt() | llm

    return {
        "question": question,
        "answers": [
            {
                "answer": __invoke_answer_chain(answers_chain, question, doc.page_content),
                "source": doc.metadata["source"],
                "date": 'None',
            } for doc in docs
        ]
    }


# _ -> ì‚¬ìš©ì ì˜ ì§ˆë¬¸ + context
def __load_memory(_):
    return memory.load_memory_variables({})["chat_history"]


def __invoke_answer_chain(chain_param, question, context):
    chain_result = chain_param.invoke(
        {"question": question, "context": context}
    )
    memory.save_context({"input": question}, {"output": chain_result.content})
    return chain_result


def __pick_answer(inputs):
    question = inputs["question"]
    answers = inputs["answers"]

    pick_chain = pick_answer_prompt() | llm

    condensed = "\n\n".join(f"Answer: {answer['answer']}\nSource: {answer['source']}\ndate: {answer['date']}\n"
                            for answer in answers)
    return pick_chain.invoke({
        "question": question,
        "answers": condensed
    })


st.title('SiteGPT')

st.markdown(
    """
    Ask Questions about the content of a website.
    
    Start by writing URL of the website on the sidebar
    """
)

# session state ëŠ” í˜ì´ì§€ ë¥¼ ë‹«ì§€ ì•ŠëŠ” ì´ìƒ ìœ ì§€ ë˜ëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— í˜„ì¬ í˜ì´ì§€ ì˜ input value ë¥¼ ê¸°ë°˜ ìœ¼ë¡œ í•œ session state ì˜ ê²½ìš°
# í˜„ì¬ í˜ì´ì§€ -> ë‹¤ë¥¸ í˜ì´ì§€ -> í˜„ì¬ í˜ì´ì§€ ë¡œ ì´ë™ì‹œ ë¬¸ì œê°€ ë°œìƒí•  ì†Œì§€ê°€ ìˆë‹¤
# https://pypi.org/e1.sitemap.xml auto, wind, django
with st.sidebar:
    url = st.text_input("Write down a URL", placeholder="https://www.google.com")
    url_filter = st.text_input("Write a filter string", placeholder="index")

    # í˜ì´ì§€ ì´ˆê¸°í™” ì‹œ url ì´ ì—†ì„ ê²½ìš° session_state ì´ˆê¸°í™”
    if not url:
        set_session('run_search_sitemap', False)

    if st.button("Search SiteMap"):
        on_button_click(url)

if st.session_state.run_search_sitemap:
    retriever = find_docs_by_sitemap(url, url_filter)

    if retriever is None:
        st.error("SiteMap Info Not Found")
        set_session('run_search_sitemap', False)
    else:
        # text_input / chat_input ì˜ ì°¨ì´ì 
        # chat_input ì€ ì „ì†¡ì„ í•˜ë©´ ì…ë ¥ ê°’ì´ ì´ˆê¸°í™” ë˜ì§€ë§Œ text_input ì€ ê°’ì´ ì´ˆê¸°í™” ì•ˆë¨
        query = st.chat_input("Ask question to the website")

        if query:
            chain = ({"docs": retriever, "question": RunnablePassthrough()}
                     | RunnableLambda(__get_answers)
                     | RunnableLambda(__pick_answer))

            result = chain.invoke(query)
            st.write(result.content)

