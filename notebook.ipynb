{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:07:02.607782Z",
     "start_time": "2024-05-13T06:06:59.962540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, List, Any\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.output_parser import T\n",
    "\n",
    "# llm = OpenAI() ## text-davinci-003 (deprecated) 2024-01 에 폐기됨 \n",
    "chat = ChatOpenAI(temperature=0.1)  ## gpt-3.5-turbo\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}?\")\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "# a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(prompt)\n",
    "\n",
    "b"
   ],
   "id": "b3ea83565a4ed499",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\py_workspace\\fullstack-gpt\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\dev\\py_workspace\\fullstack-gpt\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand is approximately 9,800 miles (15,800 kilometers) when measured in a straight line.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert And you only reply {language}\"),  # 시스템 메시지 설정 (모델 사용시 설정 조건)\n",
    "    (\"ai\", \"Ciao, mi chiamo {name}\"),  # AI 메시지 설정 (답변시 AI가 추가 하는 메시지)\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b} Also what is your name\"),  # 사용자 질의 설정\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(language=\"English\", name=\"Socrates\", country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "# message 기반의 prompt 를 사용할 때에는 predict_messages 를 사용\n",
    "c = chat.predict_messages(prompt)\n",
    "\n",
    "c"
   ],
   "id": "f0b43b71c0aeb2e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        items = text.split(\",\")\n",
    "        return list(map(str.strip, items))\n"
   ],
   "id": "13efabdc9028a4df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 일반적 으로 사용 되는 방식\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a list generating machine, Everything you are asked will be answered with a comma seperated list of max {max_items} in lowercase. Do NOT reply with anything else\"),\n",
    "    # (\"ai\", \"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(max_items=10, question=\"What are the colors?\")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content)\n"
   ],
   "id": "97db2db7adc121a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# chain 을 이용 하여 중간 단계 (template.format_messages, chat.predict_messages) 를 생략 하는 방법\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a list generating machine, Everything you are asked will be answered with a comma seperated list of max {max_items} in lowercase. Do NOT reply with anything else\"),\n",
    "    # (\"ai\", \"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# | 연산자 를 이용 하여 여러 동작을 1개로 묶는다 여러 개의 chain 을 | 연산자 를 이용 하여 묶는 것도 가능 \n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\": 10,\n",
    "    \"question\": \"What are the pokemons?\",\n",
    "})\n",
    "\n"
   ],
   "id": "c115ff9b20546bdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "chef_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients\"),\n",
    "    (\"human\", \"I want to cool {cuisine} food\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_template | chat"
   ],
   "id": "ec824a4d3a425ea2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation.\"\n",
    "     \" You don`t radically modify the recipe. If there is no alternative for a food just say you don`t know how to replace it.\"),\n",
    "    (\"human\", \"{recipie}\")\n",
    "])\n",
    "\n",
    "veg_chef_chain = veg_chef_prompt | chat\n",
    "\n",
    "# 실행은 순차 적으로 실행 된다 {\"recipie\" : chef_chain} 이렇게 정의 하면 다음 chain 의 인자로 현재 chain 이 입력 된다\n",
    "final_chain = {\"recipie\": chef_chain} | veg_chef_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",\n",
    "})"
   ],
   "id": "591ffd3fd6d6a11e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:02:14.232580Z",
     "start_time": "2024-04-22T01:02:14.217595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"France\",\n",
    "        \"answer\": \"\"\",\n",
    "            Here is what I know:\",\n",
    "            Capital: Paris\",\n",
    "            Language: French\",\n",
    "            Food: Wine and Cheese\",\n",
    "            Currency: Euro\",\n",
    "            \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Italy\",\n",
    "        \"answer\": \"\"\",\n",
    "            I know this:\",\n",
    "            Capital: Rome\",\n",
    "            Language: Italian\",\n",
    "            Food: Pizza and Pasta\",\n",
    "            Currency: Euro\",\n",
    "            \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Greece\",\n",
    "        \"answer\": \"\"\",\n",
    "            I know this:\",\n",
    "            Capital: Athens\",\n",
    "            Language: Greek\",\n",
    "            Food: Souvlaki and Feta Cheese\",\n",
    "            Currency: Euro\",\n",
    "            \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    # jupyter notebook 에서 type 을 인식 하지 못하는 문제가 있다 실제 구현 시에는 당연히 데이터 type 을 넣어야 함\n",
    "    def add_example(self, example):\n",
    "        self.data.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.data)]\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# 예제 를 무한 대로 model 에 입력할 수 없기 때문에 개수 기반 으로 예시를 model 에 입력\n",
    "example_selector = RandomExampleSelector(data=examples)\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=80\n",
    "# )\n",
    "\n",
    "# example_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"human\", \"What do you know about {question}?\"),\n",
    "#     (\"ai\", \"{answer}\")\n",
    "# ])\n",
    "\n",
    "\n",
    "# few shot 은 정형화 된 질문과 답변을 처리 하는 데 유용해 보인다 ex) ~에 대해 알려주세요\n",
    "# 실제 서비스 를 구축 하는 입장 에서는 여러 가지 prompt 를 case 별로 사용을 해야 할듯\n",
    "# example_prompt = FewShotChatMessagePromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "# )\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# final_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a geography expert, you give short answers\"),\n",
    "#     example_prompt,\n",
    "#     (\"human\", \"What do you know about {country}?\")\n",
    "# ])\n",
    "\n",
    "chains = prompt | chat\n",
    "\n",
    "chains.invoke({\n",
    "    \"country\": \"korea\"\n",
    "})\n",
    "\n"
   ],
   "id": "ac1b8cc698ad5751",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Human: France\\n    AI: ,\\n            Here is what I know:\",\\n            Capital: Paris\",\\n            Language: French\",\\n            Food: Wine and Cheese\",\\n            Currency: Euro\",\\n            \\n\\n\\nHuman: What do you know about Mexico?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:30:13.706927Z",
     "start_time": "2024-04-22T01:30:12.265592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country=\"Korea\")\n",
    "\n",
    "intro = PromptTemplate.from_template(\"\"\"\n",
    "    You are a role playing assistant\n",
    "    And you are impersonating a {character}\n",
    "\"\"\")\n",
    "\n",
    "example = PromptTemplate.from_template(\"\"\"\n",
    "    This is an example of how you talk:\n",
    "    \n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\")\n",
    "\n",
    "start = PromptTemplate.from_template(\"\"\"\n",
    "    Start now!\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\")\n",
    "\n",
    "final = PromptTemplate.from_template(\"\"\"\n",
    "    {intro}\n",
    "    \n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "\"\"\")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "full_prompt.format(\n",
    "    character=\"Pirate\",\n",
    "    example_question=\"What is your location?\",\n",
    "    example_answer=\"Arrrrg! This is a secret!!\",\n",
    "    question=\"What is your favorite food?\",\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\": \"Pirate\",\n",
    "    \"example_question\": \"What is your location?\",\n",
    "    \"example_answer\": \"Arrrrg! This is a secret!!\",\n",
    "    \"question\": \"What is your favorite food?\"\n",
    "})"
   ],
   "id": "b562726e51eec1bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrrg! Me favorite food be a good ol' plate of rum-soaked sea biscuits! Aye, nothing beats the taste of salty goodness on the high seas!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Arrrrg! Me favorite food be a good ol' plate of rum-soaked sea biscuits! Aye, nothing beats the taste of salty goodness on the high seas!\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:57:49.926440Z",
     "start_time": "2024-04-22T22:57:49.914478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# 기존 질문에 대한 답변을 caching (이 경우 에는 memory 에 저장)\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")\n",
    "\n"
   ],
   "id": "fc0cc151bc08c6ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into desired shapes, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into desired shapes, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into desired shapes, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T23:13:53.438345Z",
     "start_time": "2024-04-22T23:13:52.915291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "chat\n",
    "\n",
    "# chat = OpenAI(temperature=0.1, max_tokens=450, model=\"gtp-3.5-turbo-16k\")\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "\n",
    "# 사용량 표시 \n",
    "# with get_openai_callback() as usage:\n",
    "#     a = chat.predict(\"What is the recipe for soju\")\n",
    "#     b = chat.predict(\"What is the recipe for bread\")\n",
    "#     print(a,b,\"\\n\")\n",
    "#     print(usage)"
   ],
   "id": "e27b86b8d23e7259",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<class 'openai.api_resources.completion.Completion'>, model_name='gtp-3.5-turbo-16k', temperature=0.1, max_tokens=450, openai_api_key='sk-proj-I1hlxjpwDlgU6xYeQRvlT3BlbkFJmLaEEzGDYzKbw4zcAkuS ', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T07:45:13.192561Z",
     "start_time": "2024-04-24T07:45:13.185786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# memory - 이전 질문, 답변을 기억한 다음 다음 답변이나 재 질문 시 사용\n",
    "# model 자체 에는 memory 가 없으며 대화 식으로 구성 되는건 기존 질문+답변을 memory 에 저장 했다가 model 에 기존 질문, 답변, 신규 질문을 통째로 던지는 것 \n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "# ConversationBufferMemory : 대화 내용 전체를 저장\n",
    "# ConversationBufferWindowMemory : 대화 내용중 최신 건 일부만 저장\n",
    "# return_messages: chat model 전용, 단순 문자열을 저장하는 용도라면 False 로 설정\n",
    "# memory = ConversationBufferMemory(return_messages=True)\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "add_message(5, 5)\n",
    "add_message(6, 6)\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ],
   "id": "bf17e97461e7c44f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5'),\n",
       "  HumanMessage(content='6'),\n",
       "  AIMessage(content='6')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:04:05.995313Z",
     "start_time": "2024-04-24T23:04:04.309363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ConversationSummaryBufferMemory, ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "# memory = ConversationSummaryMemory(llm=llm) # llm 을 이용하여 기존 대화를 요약하여 저장 초기에는 원래 문자열 보다 길어질 수 있으나 저장되는 양이 커질수록 효과를 본다\n",
    "\n",
    "# 현재는 llm 을 OpenAI 를 사용 하고 있기 때문에 요금이 나가는데 이를 ollama 를 이용 하여 offline 으로 돌려서 요금을 절약하거나 또는 요약에 특화된 model을 이용하여 요약의 정확도를 높인다던가\n",
    "# 하는 방식으로 이용하는것도 가능할듯\n",
    "# memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=150, return_messages=True) # 최신 데이터는 원본 그대로 가지고 있고 예전 데이터는 요약하여 저장\n",
    "\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)  # 대화중 knowledge graph 를 생성하여 요약본을 저장 \n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I`m Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ],
   "id": "174fa775956db69d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:07:26.497649Z",
     "start_time": "2024-04-24T23:07:25.633512Z"
    }
   },
   "cell_type": "code",
   "source": "add_message(\"Nicolas likes kimchi\", \"I with I could go!!\")",
   "id": "459cc8438fdb3246",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T22:48:30.685791Z",
     "start_time": "2024-04-24T22:48:30.681276Z"
    }
   },
   "cell_type": "code",
   "source": "add_message(\"How far is Korea from Argentina?\", \"I don`t know! Super far!\")",
   "id": "bf91c8618067d058",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:07:59.551686Z",
     "start_time": "2024-04-24T23:07:58.910381Z"
    }
   },
   "cell_type": "code",
   "source": "memory.load_memory_variables({\"input\": \"what does nicolas like?\"})",
   "id": "f222318dab7a71d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas is human. Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T22:49:49.372682Z",
     "start_time": "2024-04-24T22:49:49.368549Z"
    }
   },
   "cell_type": "code",
   "source": "get_history()",
   "id": "3f3af1fcafbd7bfc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI and introduces themselves as Nicalas from South Korea.'),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I with I could go!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content='I don`t know! Super far!'),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content='I don`t know! Super far!'),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content='I don`t know! Super far!'),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content='I don`t know! Super far!')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f8e23b4ff95fd0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T23:25:11.889982Z",
     "start_time": "2024-05-18T23:25:10.797174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory_key : template 에 입력될 memory 정보를 mapping 하는 key\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# template 구성 시 ConversationSummaryBufferMemory 에 저장 되어 있는 내용을 같이 입력 하도록 설정\n",
    "template = \"\"\"\n",
    "    You are helpful AI taking to a human.\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# MessageHolder: 위 template 의 {chat_history} 와 동일한 역할을 수행\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful AI taking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# chain = LLMChain(llm=llm, memory=memory, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "def load_memory(input_param):\n",
    "    print(input_param)\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\n",
    "# RunnablePassthrough : chain 이 실행될 때 chain 에 들어 가는 변수중 일부를 자동 할당\n",
    "# chain = RunnablePassthrough.assign(chat_history=load_memory) | chat_prompt_template | llm\n",
    "chain =  chat_prompt_template | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    chain_result = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": load_memory(\"dummy param\"),\n",
    "    })\n",
    "\n",
    "    memory.save_context({\"input\": question}, {\"output\": chain_result.content})\n",
    "\n",
    "    return chain_result\n",
    "\n",
    "\n",
    "invoke_chain(\"My name is Nico\")\n",
    "\n",
    "# chain.predict(question=\"My name is Nico\")"
   ],
   "id": "571b295aa32bc7b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy param\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Nico! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 23, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7253d099-5371-4f1a-90c1-955cc33521ea-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:20:16.708816Z",
     "start_time": "2024-04-27T23:20:15.849545Z"
    }
   },
   "cell_type": "code",
   "source": "invoke_chain(\"I live in Seoul\")",
   "id": "2e21ebf632f74ebf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'I live in Seoul'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great to know! How can I assist you with information or anything else related to Seoul?\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:20:20.525899Z",
     "start_time": "2024-04-27T23:20:19.964165Z"
    }
   },
   "cell_type": "code",
   "source": "invoke_chain(\"What is my name?\")",
   "id": "5dd57d97a3277f78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is my name?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Nico.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T00:47:00.770728Z",
     "start_time": "2024-05-03T00:46:53.275558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG : memory 가 이전 대화 기록을 신규 query 에 같이 보낸 다면 RAG 는 외부 DB or 저장소 에 있는 데이터 를 신규 query 에 같이 보낸다\n",
    "# langchain 공식 홈페이지에서는 LLM 지식을 추가적인 데이터를 이용하여 강화하는 것으로 정의하고 있음 (RAG is a technique for augmenting LLM knowledge with additional data.)\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache\")\n",
    "\n",
    "# chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# chunk_size(문자를 분할할 때 크기), chunk_overlap(분할되는 이전/다음 chunk 의 데이터를 현재 데이터에 덧붙임, chunk 마다 중복되는 부분이 있을 수 있음)\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "# token : llm model 에서 문자를 불규칙한 크기로 묶은 집합 ex) hello -> [he][llo] 이런 식으로 llm 에서 분류하고 [he][llo] 자체가 token 이 된다\n",
    "# embed : 특정한 token 에 대해 n차원 (= n개의 특성) 의 각 차원 별로 평가 점수? ( = 특성에 얼마나 부합하는지 ) 를 부여 ( = vector 화 )\n",
    "# token 별로 embed 작업을 통해 값이 부여 되면 이를 가지고 연산을 통해 새로운 값을 도출 하는 것이 가능\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=600, chunk_overlap=100, separator=\"\\n\")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/sample.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cache 에 데이터 가 없으면 OpenAI 를 통해서 embedding 하지만 cache 에 값이 있으면 cache 의 값을 불러 온다\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cache_embeddings)\n",
    "\n",
    "embedd_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "embedd_chain.run(\"Describe Victory Mansions\")"
   ],
   "id": "9496a0c768a939bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\py_workspace\\fullstack-gpt\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = vectorstore.similarity_search(\"where does winston live\")\n",
    "results"
   ],
   "id": "b0f5b287d5e0832b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
