{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:07:02.607782Z",
     "start_time": "2024-05-13T06:06:59.962540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, List, Any\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.output_parser import T\n",
    "\n",
    "# llm = OpenAI() ## text-davinci-003 (deprecated) 2024-01 에 폐기됨 \n",
    "chat = ChatOpenAI(temperature=0.1)  ## gpt-3.5-turbo\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}?\")\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "# a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(prompt)\n",
    "\n",
    "b"
   ],
   "id": "b3ea83565a4ed499",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert And you only reply {language}\"),  # 시스템 메시지 설정 (모델 사용시 설정 조건)\n",
    "    (\"ai\", \"Ciao, mi chiamo {name}\"),  # AI 메시지 설정 (답변시 AI가 추가 하는 메시지)\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b} Also what is your name\"),  # 사용자 질의 설정\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(language=\"English\", name=\"Socrates\", country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "# message 기반의 prompt 를 사용할 때에는 predict_messages 를 사용\n",
    "c = chat.predict_messages(prompt)\n",
    "\n",
    "c"
   ],
   "id": "f0b43b71c0aeb2e8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        items = text.split(\",\")\n",
    "        return list(map(str.strip, items))\n"
   ],
   "id": "13efabdc9028a4df",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 일반적 으로 사용 되는 방식\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a list generating machine, Everything you are asked will be answered with a comma seperated list of max {max_items} in lowercase. Do NOT reply with anything else\"),\n",
    "    # (\"ai\", \"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(max_items=10, question=\"What are the colors?\")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content)\n"
   ],
   "id": "97db2db7adc121a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# chain 을 이용 하여 중간 단계 (template.format_messages, chat.predict_messages) 를 생략 하는 방법\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a list generating machine, Everything you are asked will be answered with a comma seperated list of max {max_items} in lowercase. Do NOT reply with anything else\"),\n",
    "    # (\"ai\", \"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# | 연산자 를 이용 하여 여러 동작을 1개로 묶는다 여러 개의 chain 을 | 연산자 를 이용 하여 묶는 것도 가능 \n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\": 10,\n",
    "    \"question\": \"What are the pokemons?\",\n",
    "})\n",
    "\n"
   ],
   "id": "c115ff9b20546bdf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "chef_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients\"),\n",
    "    (\"human\", \"I want to cool {cuisine} food\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_template | chat"
   ],
   "id": "ec824a4d3a425ea2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation.\"\n",
    "     \" You don`t radically modify the recipe. If there is no alternative for a food just say you don`t know how to replace it.\"),\n",
    "    (\"human\", \"{recipie}\")\n",
    "])\n",
    "\n",
    "veg_chef_chain = veg_chef_prompt | chat\n",
    "\n",
    "# 실행은 순차 적으로 실행 된다 {\"recipie\" : chef_chain} 이렇게 정의 하면 다음 chain 의 인자로 현재 chain 이 입력 된다\n",
    "final_chain = {\"recipie\": chef_chain} | veg_chef_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",\n",
    "})"
   ],
   "id": "591ffd3fd6d6a11e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:02:14.232580Z",
     "start_time": "2024-04-22T01:02:14.217595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"France\",\n",
    "        \"answer\": \"\"\",\n",
    "            Here is what I know:\",\n",
    "            Capital: Paris\",\n",
    "            Language: French\",\n",
    "            Food: Wine and Cheese\",\n",
    "            Currency: Euro\",\n",
    "            \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Italy\",\n",
    "        \"answer\": \"\"\",\n",
    "            I know this:\",\n",
    "            Capital: Rome\",\n",
    "            Language: Italian\",\n",
    "            Food: Pizza and Pasta\",\n",
    "            Currency: Euro\",\n",
    "            \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Greece\",\n",
    "        \"answer\": \"\"\",\n",
    "            I know this:\",\n",
    "            Capital: Athens\",\n",
    "            Language: Greek\",\n",
    "            Food: Souvlaki and Feta Cheese\",\n",
    "            Currency: Euro\",\n",
    "            \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    # jupyter notebook 에서 type 을 인식 하지 못하는 문제가 있다 실제 구현 시에는 당연히 데이터 type 을 넣어야 함\n",
    "    def add_example(self, example):\n",
    "        self.data.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.data)]\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# 예제 를 무한 대로 model 에 입력할 수 없기 때문에 개수 기반 으로 예시를 model 에 입력\n",
    "example_selector = RandomExampleSelector(data=examples)\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=80\n",
    "# )\n",
    "\n",
    "# example_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"human\", \"What do you know about {question}?\"),\n",
    "#     (\"ai\", \"{answer}\")\n",
    "# ])\n",
    "\n",
    "\n",
    "# few shot 은 정형화 된 질문과 답변을 처리 하는 데 유용해 보인다 ex) ~에 대해 알려주세요\n",
    "# 실제 서비스 를 구축 하는 입장 에서는 여러 가지 prompt 를 case 별로 사용을 해야 할듯\n",
    "# example_prompt = FewShotChatMessagePromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "# )\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# final_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a geography expert, you give short answers\"),\n",
    "#     example_prompt,\n",
    "#     (\"human\", \"What do you know about {country}?\")\n",
    "# ])\n",
    "\n",
    "chains = prompt | chat\n",
    "\n",
    "chains.invoke({\n",
    "    \"country\": \"korea\"\n",
    "})\n",
    "\n"
   ],
   "id": "ac1b8cc698ad5751",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:30:13.706927Z",
     "start_time": "2024-04-22T01:30:12.265592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country=\"Korea\")\n",
    "\n",
    "intro = PromptTemplate.from_template(\"\"\"\n",
    "    You are a role playing assistant\n",
    "    And you are impersonating a {character}\n",
    "\"\"\")\n",
    "\n",
    "example = PromptTemplate.from_template(\"\"\"\n",
    "    This is an example of how you talk:\n",
    "    \n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\")\n",
    "\n",
    "start = PromptTemplate.from_template(\"\"\"\n",
    "    Start now!\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\")\n",
    "\n",
    "final = PromptTemplate.from_template(\"\"\"\n",
    "    {intro}\n",
    "    \n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "\"\"\")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "full_prompt.format(\n",
    "    character=\"Pirate\",\n",
    "    example_question=\"What is your location?\",\n",
    "    example_answer=\"Arrrrg! This is a secret!!\",\n",
    "    question=\"What is your favorite food?\",\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\": \"Pirate\",\n",
    "    \"example_question\": \"What is your location?\",\n",
    "    \"example_answer\": \"Arrrrg! This is a secret!!\",\n",
    "    \"question\": \"What is your favorite food?\"\n",
    "})"
   ],
   "id": "b562726e51eec1bf",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:57:49.926440Z",
     "start_time": "2024-04-22T22:57:49.914478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# 기존 질문에 대한 답변을 caching (이 경우 에는 memory 에 저장)\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")\n",
    "\n"
   ],
   "id": "fc0cc151bc08c6ac",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T23:13:53.438345Z",
     "start_time": "2024-04-22T23:13:52.915291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "chat\n",
    "\n",
    "# chat = OpenAI(temperature=0.1, max_tokens=450, model=\"gtp-3.5-turbo-16k\")\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "\n",
    "# 사용량 표시 \n",
    "# with get_openai_callback() as usage:\n",
    "#     a = chat.predict(\"What is the recipe for soju\")\n",
    "#     b = chat.predict(\"What is the recipe for bread\")\n",
    "#     print(a,b,\"\\n\")\n",
    "#     print(usage)"
   ],
   "id": "e27b86b8d23e7259",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T07:45:13.192561Z",
     "start_time": "2024-04-24T07:45:13.185786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# memory - 이전 질문, 답변을 기억한 다음 다음 답변이나 재 질문 시 사용\n",
    "# model 자체 에는 memory 가 없으며 대화 식으로 구성 되는건 기존 질문+답변을 memory 에 저장 했다가 model 에 기존 질문, 답변, 신규 질문을 통째로 던지는 것 \n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "# ConversationBufferMemory : 대화 내용 전체를 저장\n",
    "# ConversationBufferWindowMemory : 대화 내용중 최신 건 일부만 저장\n",
    "# return_messages: chat model 전용, 단순 문자열을 저장하는 용도라면 False 로 설정\n",
    "# memory = ConversationBufferMemory(return_messages=True)\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "add_message(5, 5)\n",
    "add_message(6, 6)\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ],
   "id": "bf17e97461e7c44f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:04:05.995313Z",
     "start_time": "2024-04-24T23:04:04.309363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ConversationSummaryBufferMemory, ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "# memory = ConversationSummaryMemory(llm=llm) # llm 을 이용하여 기존 대화를 요약하여 저장 초기에는 원래 문자열 보다 길어질 수 있으나 저장되는 양이 커질수록 효과를 본다\n",
    "\n",
    "# 현재는 llm 을 OpenAI 를 사용 하고 있기 때문에 요금이 나가는데 이를 ollama 를 이용 하여 offline 으로 돌려서 요금을 절약하거나 또는 요약에 특화된 model을 이용하여 요약의 정확도를 높인다던가\n",
    "# 하는 방식으로 이용하는것도 가능할듯\n",
    "# memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=150, return_messages=True) # 최신 데이터는 원본 그대로 가지고 있고 예전 데이터는 요약하여 저장\n",
    "\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)  # 대화중 knowledge graph 를 생성하여 요약본을 저장 \n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I`m Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ],
   "id": "174fa775956db69d",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:07:26.497649Z",
     "start_time": "2024-04-24T23:07:25.633512Z"
    }
   },
   "cell_type": "code",
   "source": "add_message(\"Nicolas likes kimchi\", \"I with I could go!!\")",
   "id": "459cc8438fdb3246",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T22:48:30.685791Z",
     "start_time": "2024-04-24T22:48:30.681276Z"
    }
   },
   "cell_type": "code",
   "source": "add_message(\"How far is Korea from Argentina?\", \"I don`t know! Super far!\")",
   "id": "bf91c8618067d058",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:07:59.551686Z",
     "start_time": "2024-04-24T23:07:58.910381Z"
    }
   },
   "cell_type": "code",
   "source": "memory.load_memory_variables({\"input\": \"what does nicolas like?\"})",
   "id": "f222318dab7a71d6",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T22:49:49.372682Z",
     "start_time": "2024-04-24T22:49:49.368549Z"
    }
   },
   "cell_type": "code",
   "source": "get_history()",
   "id": "3f3af1fcafbd7bfc",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "1f8e23b4ff95fd0e",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T23:25:11.889982Z",
     "start_time": "2024-05-18T23:25:10.797174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory_key : template 에 입력될 memory 정보를 mapping 하는 key\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# template 구성 시 ConversationSummaryBufferMemory 에 저장 되어 있는 내용을 같이 입력 하도록 설정\n",
    "template = \"\"\"\n",
    "    You are helpful AI taking to a human.\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# MessageHolder: 위 template 의 {chat_history} 와 동일한 역할을 수행\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful AI taking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# chain = LLMChain(llm=llm, memory=memory, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "def load_memory(input_param):\n",
    "    print(input_param)\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\n",
    "# RunnablePassthrough : chain 이 실행될 때 chain 에 들어 가는 변수중 일부를 자동 할당\n",
    "# chain = RunnablePassthrough.assign(chat_history=load_memory) | chat_prompt_template | llm\n",
    "chain =  chat_prompt_template | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    chain_result = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": load_memory(\"dummy param\"),\n",
    "    })\n",
    "\n",
    "    memory.save_context({\"input\": question}, {\"output\": chain_result.content})\n",
    "\n",
    "    return chain_result\n",
    "\n",
    "\n",
    "invoke_chain(\"My name is Nico\")\n",
    "\n",
    "# chain.predict(question=\"My name is Nico\")"
   ],
   "id": "571b295aa32bc7b9",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:20:16.708816Z",
     "start_time": "2024-04-27T23:20:15.849545Z"
    }
   },
   "cell_type": "code",
   "source": "invoke_chain(\"I live in Seoul\")",
   "id": "2e21ebf632f74ebf",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:20:20.525899Z",
     "start_time": "2024-04-27T23:20:19.964165Z"
    }
   },
   "cell_type": "code",
   "source": "invoke_chain(\"What is my name?\")",
   "id": "5dd57d97a3277f78",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T00:47:00.770728Z",
     "start_time": "2024-05-03T00:46:53.275558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG : memory 가 이전 대화 기록을 신규 query 에 같이 보낸 다면 RAG 는 외부 DB or 저장소 에 있는 데이터 를 신규 query 에 같이 보낸다\n",
    "# langchain 공식 홈페이지에서는 LLM 지식을 추가적인 데이터를 이용하여 강화하는 것으로 정의하고 있음 (RAG is a technique for augmenting LLM knowledge with additional data.)\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache\")\n",
    "\n",
    "# chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# chunk_size(문자를 분할할 때 크기), chunk_overlap(분할되는 이전/다음 chunk 의 데이터를 현재 데이터에 덧붙임, chunk 마다 중복되는 부분이 있을 수 있음)\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "# token : llm model 에서 문자를 불규칙한 크기로 묶은 집합 ex) hello -> [he][llo] 이런 식으로 llm 에서 분류하고 [he][llo] 자체가 token 이 된다\n",
    "# embed : 특정한 token 에 대해 n차원 (= n개의 특성) 의 각 차원 별로 평가 점수? ( = 특성에 얼마나 부합하는지 ) 를 부여 ( = vector 화 )\n",
    "# token 별로 embed 작업을 통해 값이 부여 되면 이를 가지고 연산을 통해 새로운 값을 도출 하는 것이 가능\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=600, chunk_overlap=100, separator=\"\\n\")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/sample.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cache 에 데이터 가 없으면 OpenAI 를 통해서 embedding 하지만 cache 에 값이 있으면 cache 의 값을 불러 온다\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cache_embeddings)\n",
    "\n",
    "embedd_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "embedd_chain.run(\"Describe Victory Mansions\")"
   ],
   "id": "9496a0c768a939bc",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = vectorstore.similarity_search(\"where does winston live\")\n",
    "results"
   ],
   "id": "b0f5b287d5e0832b",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
